
## ğŸ”— Architectural Pattern
**Retrieval-Augmented Generation (RAG)** with strict document grounding

---

## ğŸ§© Component Breakdown

### ğŸ“‚ Amazon S3 â€” Source Layer
- Stores all product and client documents
- Organized using prefixes:
  - `product_docs/`
  - `client_docs/`

---

### ğŸ§  Amazon Bedrock â€” Intelligence Layer
- Manages:
  - Document ingestion
  - Chunking
  - Embedding generation
  - Retrieval orchestration
- Embedding Model: **amazon-titan-embed-text-v2**

---

### ğŸ” OpenSearch Serverless â€” Vector Store
- Stores embeddings
- Performs semantic similarity search
- Fully managed, no cluster operations

---

### ğŸ’¬ Amazon Lex â€” Conversation Layer
- Handles:
  - Intent recognition
  - Utterance routing
  - Structured dialogue
- Delegates retrieval & generation to Bedrock

---

## ğŸ” Runtime Flow

1ï¸âƒ£ User asks a question in Lex  
2ï¸âƒ£ Intent detected  
3ï¸âƒ£ Query sent to Bedrock  
4ï¸âƒ£ Relevant chunks retrieved from OpenSearch  
5ï¸âƒ£ LLM generates response using retrieved context only  
6ï¸âƒ£ Answer returned with grounded reasoning  

---

## âœ… Why This Architecture Works for Finance

âœ” Prevents hallucinations  
âœ” Enforces compliance boundaries  
âœ” Supports explainability  
âœ” Scales without infra overhead  
